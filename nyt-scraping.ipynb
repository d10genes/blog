{
 "metadata": {
  "name": "nyt-scraping"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "{\"Title\": \"NYT nlp\",\n",
      "\"Date\": \"2013-7-4\",\n",
      "\"Category\": \"ipython\",\n",
      "\"Tags\": \"nlp, ipython\",\n",
      "\"slug\": \"slug-slug-slug\",\n",
      "\"Author\": \"Chris\"\n",
      "}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This post uses the [New York Times API](http://developer.nytimes.com/docs/read/article_search_api_v2) to search for articles on US politics that include the word *scandal*, and several python libraries to grab the text of those articles and store them to MongoDB for some natural language processing analytics."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These commands will install some of the dependencies for this project:\n",
      "\n",
      "    pip install pymongo\n",
      "    pip install requests\n",
      "    pip install lxml\n",
      "    pip install cssselect"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import json\n",
      "from time import sleep\n",
      "from itertools import count\n",
      "from lxml.cssselect import CSSSelector\n",
      "from lxml.html import fromstring\n",
      "import pymongo\n",
      "import datetime as dt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autosave\n",
      "%autosave 30"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Mongodb"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to connect to the database, assuming it's already running (`mongod` from the terminal)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "connection = pymongo.Connection(\"localhost\", 27017 )\n",
      "db = connection.nyt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Get URLs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After using your secret API key..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from key import apikey\n",
      "apiparams = {'api-key': apikey}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "...the first thing we need to get is the urls for all the articles that match our search criterion. This is a bit convoluted, since I couldn't find a way to search for *republican OR democrat*, so I ended up just repeating the query both times. I found out that there are lots of really interesting curated details you can use in the search, such as searching for articles pertaining to certain geographic areas, people or organizations. I used some of these features to narrow the results down to the US, restricted the dates to between 1992-2013, and just asked for title, URL and date to use as unique identifiers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "page = 0\n",
      "party = 'republican'\n",
      "#party = 'democrat'\n",
      "\n",
      "q = 'http://api.nytimes.com/svc/search/v1/article?'\n",
      "params = {'query': 'body:scandal+{} geo_facet:[UNITED STATES]'.format(party),\n",
      "            'begin_date': '19920101',\n",
      "            'end_date': '20131201',\n",
      "            'fields': 'title,url,date',\n",
      "            'offset': page,\n",
      "            'api-key': apikey\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After constructing the query, we grab the search results with [requests](http://docs.python-requests.org/en/latest/). There's no way to tell how many results there will be, so we go as long as we can, shoving everything into MongoDB, incrementing the `offset` query parameter and pausing for a break before the next page of results (the NYT has a limit on how many times you can query them per second)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdate = lambda d: dt.datetime.strptime(d, '%Y%m%d')\n",
      "\n",
      "for page in count():  #keep looping indefinitely\n",
      "    params.update({'offset': page})  #fetch another ten results from the next page\n",
      "    r = requests.get(q, params=params)\n",
      "    res = json.loads(r.content)[\"results\"]\n",
      "    if res:\n",
      "        for dct in res:\n",
      "            url = dct.pop('url')\n",
      "            dct['date'] = fdate(dct['date'])  #string -> format as datetime object\n",
      "            db.raw_text.update({'url': url}, {'$set': dct}, upsert=True)\n",
      "    else:  #no more results\n",
      "        break\n",
      "    print page,\n",
      "    sleep(.2)\n",
      "#urls = {r['url']: r for r in url_lst}    \n",
      "#del url_lst"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "12 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "16 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "17 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "18 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "19 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "21 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "22 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "23 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "24 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "25 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "26 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "27 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "28 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "29 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "30 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "31 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "32 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "33 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "34 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "35 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "36 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "37 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "38 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "39 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "40 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "41 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "42 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "43 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "44 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "45 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "46 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "47 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "48 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "49 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "51 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "52 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "53 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "54 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "55 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "56 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "57 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "58 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "59 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "60 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "61 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "62 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "63 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "64 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "65 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "66 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "67 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "68 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "69 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "70 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "71 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "72 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "73 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "74 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "75 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "76 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "77 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "78\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Scrape Text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are a few of the resulting URLS that we'll use to get the full text articles:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[doc['url'] for doc in db.raw_text.find()][:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[u'http://www.nytimes.com/2007/08/15/opinion/15wed3.html',\n",
        " u'http://www.nytimes.com/2007/03/16/washington/16cong.html',\n",
        " u'http://www.nytimes.com/2006/03/11/politics/11lobby.html',\n",
        " u'http://www.nytimes.com/2006/04/30/washington/30cunningham.html',\n",
        " u'http://www.nytimes.com/2004/04/09/business/senate-panel-asked-to-give-sec-proposals-a-chance.html']"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The scraping wasn't as difficult as I was expecting; over the 20 or so years that I searched for, the body text of the articles could be found by looking at 5 html elements (formatted as CSS selectors in `_sels` below). The following two functions do most of the scraping work-- `get_text`...well...gets the text from the `CSSSelector` parser, and the `grab_text` uses this after pulling the html with requests again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_text(e):\n",
      "    \"Function to extract text from CSSSelector results\"\n",
      "    try:\n",
      "        return ' '.join(e.itertext()).strip().encode('ascii', 'ignore')\n",
      "    except UnicodeDecodeError:\n",
      "        return ''\n",
      "\n",
      "\n",
      "def grab_text(url, verbose=True):\n",
      "    \"Main scraping function--given url, grabs html, looks for and returns article text\"\n",
      "    if verbose:  #page counter\n",
      "        print grab_text.c,\n",
      "    grab_text.c += 1\n",
      "    r = requests.get(url, params=all_pages)\n",
      "    content = fromstring(r.content)\n",
      "    for _sel in _sels:\n",
      "        text_elems = CSSSelector(_sel)(content)\n",
      "        if text_elems:\n",
      "            return '\\n'.join(map(get_text, text_elems))\n",
      "    return ''\n",
      "\n",
      "#Selectors where text of articles can be found; quite a few patterns among NYT articles\n",
      "_sels =  ['p[itemprop=\"articleBody\"]', \"div.blurb-text\", 'div#articleBody p', 'div.articleBody p', 'div.mod-nytimesarticletext p']\n",
      "all_pages = {'pagewanted': 'all'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And here is the main loop and counter. Pretty simple, huh?\n",
      "\n",
      "One the first run, the output counts up from zero, but since political scandals seem to be popping up by the hour, I've updated the search a few times, but only pulling articles that aren't already in the database (hence the mostly underscored output below)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grab_text.c = 0\n",
      "for doc in db.raw_text.find():\n",
      "    if ('url' in doc) and ('text' not in doc):\n",
      "        # if we don't already have this in mongodb\n",
      "        txt = grab_text(doc['url'])\n",
      "        db.raw_text.update({'url': doc['url']}, {'$set': {'text': txt}})\n",
      "    else:\n",
      "        print '_',\n",
      "\n",
      "db.raw_text.remove({'text': u''})  #there was one weird result that didn't have any text..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And, we got more than 870 scandalous stories...but still counting."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(list(db.raw_text.find()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "874"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Conclusion\n",
      "Though it turned out to be pretty brief, I thought this first part of my NYT scandals project deserved its own post.\n",
      "Luckily it doesn't take too much effort or space when you're working with a nice, expressive language, though.\n",
      "And you can reproduce this for yourself--you can find a copy of this notebook on github."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}